{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Download CORnet-S model\n",
        "!pip uninstall -y cornet\n",
        "!pip install git+https://github.com/dicarlolab/CORnet\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LLF52md3jvT",
        "outputId": "76646c27-0cad-40bf-e930-456bd44abee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping cornet as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/dicarlolab/CORnet\n",
            "  Cloning https://github.com/dicarlolab/CORnet to /tmp/pip-req-build-9otluyux\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dicarlolab/CORnet /tmp/pip-req-build-9otluyux\n",
            "  Resolved https://github.com/dicarlolab/CORnet to commit d0cc17d4b34ad44dedb01683b70eafd15515adad\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (4.67.1)\n",
            "Collecting fire (from CORnet==0.1.0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=0.4.0->CORnet==0.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=0.4.0->CORnet==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->CORnet==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->CORnet==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->CORnet==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->CORnet==0.1.0) (2025.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->CORnet==0.1.0) (11.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->CORnet==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=0.4.0->CORnet==0.1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: CORnet, fire\n",
            "  Building wheel for CORnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for CORnet: filename=CORnet-0.1.0-py3-none-any.whl size=23226 sha256=0c24d13c07fb78b6798776f303f23d8ad93312326b31d24c5dad009146cf512b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2o0qddff/wheels/3f/1b/1e/9ab1c622b7b6a87632001d7ac43f1957f53faa0a6b83f3e850\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=a5a2d42989c7d40ba676a98d9c6d1b6a522a17ef352dc89d08c95d29179f8945\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built CORnet fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, CORnet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed CORnet-0.1.0 fire-0.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK_zsrrrr5hb",
        "outputId": "8446b170-5fae-4998-f6be-6b8a3d75ccf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from cornet.cornet_s import CORblock_S\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment\"\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/LFWCrop_dataset_pytorch\"\n",
        "PREVIOUS_MODELS_PATH = \"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Forth Experiment/models/pretrained/full_finetune\"\n",
        "\n",
        "MODEL_DIR = os.path.join(BASE_PATH, \"models\")\n",
        "OUTPUT_DIR = os.path.join(BASE_PATH, \"output\")\n",
        "\n",
        "def ensure_directories():\n",
        "    dirs = [\n",
        "        MODEL_DIR,\n",
        "        OUTPUT_DIR,\n",
        "        os.path.join(OUTPUT_DIR, \"training_curves\"),\n",
        "        os.path.join(OUTPUT_DIR, \"predictions\"),\n",
        "        os.path.join(OUTPUT_DIR, \"saliency\"),\n",
        "        os.path.join(OUTPUT_DIR, \"comparisons\"),\n",
        "        os.path.join(MODEL_DIR, \"hybrid\")\n",
        "    ]\n",
        "    for directory in dirs:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    print(\"All directory prepared\")\n",
        "\n",
        "ensure_directories()\n",
        "\n",
        "# Preopare to read the image pair\n",
        "def read_pairs(file_path, label):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [(line.strip().split()[0], line.strip().split()[1], label) for line in lines]\n",
        "\n",
        "def load_all_pairs(list_dir):\n",
        "    all_pairs = []\n",
        "    for i in range(1, 11):\n",
        "        prefix = f\"{i:02d}\"\n",
        "        for split in [\"train\", \"test\"]:\n",
        "            same_file = os.path.join(list_dir, f\"{prefix}_{split}_same.txt\")\n",
        "            diff_file = os.path.join(list_dir, f\"{prefix}_{split}_diff.txt\")\n",
        "            all_pairs += read_pairs(same_file, 1)\n",
        "            all_pairs += read_pairs(diff_file, 0)\n",
        "    return all_pairs\n",
        "\n",
        "class FacePairsDataset(Dataset):\n",
        "    def __init__(self, pairs, image_dir, transform=None):\n",
        "        self.pairs = pairs\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform or transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name1, name2, label = self.pairs[idx]\n",
        "        name1 += \".ppm\"\n",
        "        name2 += \".ppm\"\n",
        "        img1 = Image.open(os.path.join(self.image_dir, name1)).convert(\"RGB\")\n",
        "        img2 = Image.open(os.path.join(self.image_dir, name2)).convert(\"RGB\")\n",
        "        return self.transform(img1), self.transform(img2), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "def get_data_loaders():\n",
        "    face_dir = os.path.join(DATA_ROOT, \"faces\")\n",
        "    list_dir = os.path.join(DATA_ROOT, \"lists\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    pairs = load_all_pairs(list_dir)\n",
        "    split_idx = int(0.9 * len(pairs))\n",
        "    train_pairs = pairs[:split_idx]\n",
        "    val_pairs = pairs[split_idx:]\n",
        "\n",
        "    train_dataset = FacePairsDataset(train_pairs, face_dir, transform=transform)\n",
        "    val_dataset = FacePairsDataset(val_pairs, face_dir, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, train_dataset, val_dataset\n",
        "\n",
        "# Hybrid CORnet Model Design\n",
        "class HybridCORnetEmbedding(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        # Use Resnet18 as the backbone\n",
        "        self.resnet = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        # Use the 002 model (1 V1, 1 V2, 1 V4, 2 IT)\n",
        "        self.v2_time = 0\n",
        "        self.v4_time = 0\n",
        "        self.it_time = 2\n",
        "\n",
        "        # IT block\n",
        "        if self.it_time > 1:\n",
        "            self.IT_recurrent = CORblock_S(512, 512, times=self.it_time-1)\n",
        "\n",
        "        # HED structure (side conv)\n",
        "        self.side1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=1),  # post V1 feature\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.side2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=1),  # post V2 feature\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.side3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=1),  # post V4 feature\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Feature Fusion\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(384, 512, kernel_size=1),  # 128x3 combined three features\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Attention Weights\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),  # 512 backone + 512 fusion\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Sigmoid()  # Generate weight attention between 0 - 1\n",
        "        )\n",
        "\n",
        "        # Pooling\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # V1: conv1 + bn1 + relu + maxpool\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x1 = self.resnet.maxpool(x)\n",
        "\n",
        "        # V2: layer1\n",
        "        x2 = self.resnet.layer1(x1)\n",
        "\n",
        "        # V4: layer2\n",
        "        x3 = self.resnet.layer2(x2)\n",
        "\n",
        "        # IT: layer3 + layer4\n",
        "        x = self.resnet.layer3(x3)\n",
        "        x = self.resnet.layer4(x)\n",
        "        if self.it_time > 1:\n",
        "            x = self.IT_recurrent(x)\n",
        "\n",
        "        side1_feat = self.side1(x1)\n",
        "        side2_feat = self.side2(x2)\n",
        "        side3_feat = self.side3(x3)\n",
        "\n",
        "        side1_feat = F.adaptive_avg_pool2d(side1_feat, x.size()[2:])\n",
        "        side2_feat = F.adaptive_avg_pool2d(side2_feat, x.size()[2:])\n",
        "        side3_feat = F.adaptive_avg_pool2d(side3_feat, x.size()[2:])\n",
        "\n",
        "        fusion_feat = torch.cat([side1_feat, side2_feat, side3_feat], dim=1)\n",
        "        fusion_feat = self.fusion(fusion_feat)\n",
        "\n",
        "        combined = torch.cat([x, fusion_feat], dim=1)\n",
        "        attention_weights = self.attention(combined)\n",
        "\n",
        "        output = x * attention_weights + fusion_feat * (1 - attention_weights)\n",
        "\n",
        "        output = self.pool(output)\n",
        "        output = self.flatten(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Siamese\n",
        "class SiameseHybridCORnet(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.embedding_net = HybridCORnetEmbedding(pretrained=pretrained)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        f1 = self.embedding_net(x1)\n",
        "        f2 = self.embedding_net(x2)\n",
        "        return f1, f2\n",
        "\n",
        "class PretrainedCORnetEmbedding(nn.Module):\n",
        "    def __init__(self, times_dict, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        def get_times(region): return times_dict.get(region, 2)\n",
        "        self.v2_time = get_times('V2')\n",
        "        self.v4_time = get_times('V4')\n",
        "        self.it_time = get_times('IT')\n",
        "\n",
        "        # V2: layer1输出 64\n",
        "        if self.v2_time > 1:\n",
        "            self.V2_recurrent = CORblock_S(64, 64, times=self.v2_time-1)\n",
        "\n",
        "        # V4: layer2 128\n",
        "        if self.v4_time > 1:\n",
        "            self.V4_recurrent = CORblock_S(128, 128, times=self.v4_time-1)\n",
        "\n",
        "        # IT: layer4 512\n",
        "        if self.it_time > 1:\n",
        "            self.IT_recurrent = CORblock_S(512, 512, times=self.it_time-1)\n",
        "\n",
        "        # Resnet Pooling\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # V1: conv1 + bn1 + relu + maxpool\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "\n",
        "        # V2: layer1 (BasicBlock x2)\n",
        "        x = self.resnet.layer1(x)\n",
        "        if self.v2_time > 1:\n",
        "            x = self.V2_recurrent(x)\n",
        "\n",
        "        # V4: layer2 (BasicBlock x2)\n",
        "        x = self.resnet.layer2(x)\n",
        "        if self.v4_time > 1:\n",
        "            x = self.V4_recurrent(x)\n",
        "\n",
        "        # IT: layer3 (BasicBlock x2) + layer4 (BasicBlock x2)\n",
        "        x = self.resnet.layer3(x)\n",
        "        x = self.resnet.layer4(x)\n",
        "        if self.it_time > 1:\n",
        "            x = self.IT_recurrent(x)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SiamesePretrainedCORnet(nn.Module):\n",
        "    def __init__(self, times_dict, pretrained=True, freeze_backbone=False):\n",
        "        super().__init__()\n",
        "        self.embedding_net = PretrainedCORnetEmbedding(times_dict, pretrained=pretrained)\n",
        "\n",
        "        if freeze_backbone:\n",
        "            for param in self.embedding_net.resnet.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            if hasattr(self.embedding_net, 'V2_recurrent'):\n",
        "                for param in self.embedding_net.V2_recurrent.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            if hasattr(self.embedding_net, 'V4_recurrent'):\n",
        "                for param in self.embedding_net.V4_recurrent.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            if hasattr(self.embedding_net, 'IT_recurrent'):\n",
        "                for param in self.embedding_net.IT_recurrent.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        f1 = self.embedding_net(x1)\n",
        "        f2 = self.embedding_net(x2)\n",
        "        return f1, f2\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        loss = (label) * torch.pow(euclidean_distance, 2) + \\\n",
        "               (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
        "        return loss.mean()\n",
        "\n",
        "def compute_accuracy(out1, out2, labels, threshold=0.5):\n",
        "    distances = F.pairwise_distance(out1, out2)\n",
        "    preds = (distances < threshold).float()\n",
        "    correct = (preds == labels).float().sum()\n",
        "    accuracy = correct / labels.size(0)\n",
        "    return accuracy.item()\n",
        "\n",
        "def train_hybrid_model(num_epochs=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using devices: {device}\")\n",
        "\n",
        "    train_loader, val_loader, _, _ = get_data_loaders()\n",
        "\n",
        "    model = SiameseHybridCORnet(pretrained=True).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Hybrid model parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Frozen Proportion: {100 * (total_params - trainable_params) / total_params:.2f}%\")\n",
        "\n",
        "    save_model_parameters_stats(model, \"hybrid\")\n",
        "\n",
        "    criterion = ContrastiveLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    hybrid_model_dir = os.path.join(MODEL_DIR, \"hybrid\")\n",
        "    epoch_data_dir = os.path.join(hybrid_model_dir, \"epoch_data\")\n",
        "    os.makedirs(hybrid_model_dir, exist_ok=True)\n",
        "    os.makedirs(epoch_data_dir, exist_ok=True)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        for img1, img2, label in train_loader:\n",
        "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                out1, out2 = model(img1, img2)\n",
        "                loss = criterion(out1, out2, label)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            acc = compute_accuracy(out1, out2, label)\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = running_acc / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for img1, img2, label in val_loader:\n",
        "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    out1, out2 = model(img1, img2)\n",
        "                    loss = criterion(out1, out2, label)\n",
        "\n",
        "                acc = compute_accuracy(out1, out2, label)\n",
        "                running_loss += loss.item()\n",
        "                running_acc += acc\n",
        "\n",
        "        val_loss = running_loss / len(val_loader)\n",
        "        val_acc = running_acc / len(val_loader)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train loss: {train_loss:.5f}, Train acc: {train_acc:.5f} | Val loss: {val_loss:.5f}, Val acc: {val_acc:.5f}\")\n",
        "\n",
        "        epoch_results = {\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accuracies,\n",
        "            'val_accs': val_accuracies,\n",
        "            'current_epoch': epoch + 1\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(epoch_data_dir, f\"epoch_{epoch+1}_data.json\"), 'w') as f:\n",
        "            json.dump(epoch_results, f, indent=2)\n",
        "\n",
        "        if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1 or val_acc > best_val_acc:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'val_acc': val_acc\n",
        "            }\n",
        "\n",
        "            checkpoint_path = os.path.join(hybrid_model_dir, f\"checkpoint_epoch{epoch+1}.pt\")\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f\"Check point saves: {checkpoint_path}\")\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_epoch = epoch + 1\n",
        "                best_model_path = os.path.join(hybrid_model_dir, \"best_model.pt\")\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"Find new best (Val acc: {val_acc:.5f}), saved as: {best_model_path}\")\n",
        "\n",
        "    plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, \"hybrid\")\n",
        "\n",
        "    final_model_path = os.path.join(hybrid_model_dir, \"hybrid_model.pt\")\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"Final model is saved in: {final_model_path}\")\n",
        "\n",
        "    report = {\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"final_train_loss\": train_losses[-1],\n",
        "        \"final_val_loss\": val_losses[-1],\n",
        "        \"final_train_acc\": train_accuracies[-1],\n",
        "        \"final_val_acc\": val_accuracies[-1],\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"total_params\": total_params,\n",
        "        \"trainable_params\": trainable_params\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(hybrid_model_dir, \"training_report.json\"), 'w') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_model_parameters_stats(model, model_name=\"hybrid\"):\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    frozen_params = total_params - trainable_params\n",
        "\n",
        "    region_params = {}\n",
        "\n",
        "    if model_name == \"hybrid\":\n",
        "        resnet_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                         if 'embedding_net.resnet' in name and 'side' not in name\n",
        "                         and 'fusion' not in name and 'attention' not in name)\n",
        "        side_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                       if 'side' in name)\n",
        "        fusion_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                         if 'fusion' in name)\n",
        "        attention_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                           if 'attention' in name)\n",
        "        it_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                     if 'IT_recurrent' in name)\n",
        "\n",
        "        region_params = {\n",
        "            'ResNet backbone': resnet_params,\n",
        "            'Side conv': side_params,\n",
        "            'Feature Fusion': fusion_params,\n",
        "            'Attention weights': attention_params,\n",
        "            'IT recurrent': it_params\n",
        "        }\n",
        "    else:\n",
        "        resnet_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                         if 'embedding_net.resnet' in name)\n",
        "        v2_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                     if 'V2_recurrent' in name)\n",
        "        v4_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                     if 'V4_recurrent' in name)\n",
        "        it_params = sum(p.numel() for name, p in model.named_parameters()\n",
        "                     if 'IT_recurrent' in name)\n",
        "\n",
        "        region_params = {\n",
        "            'ResNet backbone': resnet_params,\n",
        "            'V2 recurrent': v2_params,\n",
        "            'V4 recurrent': v4_params,\n",
        "            'IT recurrent': it_params\n",
        "        }\n",
        "\n",
        "    stats_dir = os.path.join(MODEL_DIR, model_name)\n",
        "    os.makedirs(stats_dir, exist_ok=True)\n",
        "\n",
        "    stats = {\n",
        "        'model_name': model_name,\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'frozen_params': frozen_params,\n",
        "        'frozen_ratio': frozen_params / total_params if total_params > 0 else 0,\n",
        "        'region_params': region_params\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(stats_dir, \"model_parameters_stats.json\"), 'w', encoding='utf-8') as f:\n",
        "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    with open(os.path.join(stats_dir, \"model_parameters_stats.txt\"), 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# {model_name} Model Parameter Statistics\\n\\n\")\n",
        "        f.write(f\"Total parameters: {total_params:,}\\n\")\n",
        "        f.write(f\"Trainable parameters: {trainable_params:,}\\n\")\n",
        "        f.write(f\"Frozen parameters: {frozen_params:,}\\n\")\n",
        "        f.write(f\"Frozen proportion: {100 * frozen_params / total_params:.2f}%\\n\\n\")\n",
        "\n",
        "        f.write(\"## Parameter distribution\\n\\n\")\n",
        "        for region, count in region_params.items():\n",
        "            if count > 0:\n",
        "                f.write(f\"- {region}: {count:,} ({count/total_params*100:.2f}%)\\n\")\n",
        "\n",
        "    return stats\n",
        "\n",
        "def plot_training_curves(train_losses, val_losses, train_accs, val_accs, model_name=\"hybrid\"):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Training loss')\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation loss')\n",
        "    plt.title(f\"{model_name} Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, [acc * 100 for acc in train_accs], 'b-', label='Training accuracy')\n",
        "    plt.plot(epochs, [acc * 100 for acc in val_accs], 'r-', label='Validation accuracy')\n",
        "    plt.title(f\"{model_name} Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    curves_dir = os.path.join(OUTPUT_DIR, \"training_curves\")\n",
        "    os.makedirs(curves_dir, exist_ok=True)\n",
        "    save_path = os.path.join(curves_dir, f\"{model_name}_training_curves.png\")\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    print(f\"Training curve saved: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "def predict_models_comparison(hybrid_model_path):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    image_pairs = [\n",
        "        (\"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion.jpg\",\n",
        "         \"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion_n.jpg\",\n",
        "         \"Part_Whole_Illusion\"),\n",
        "\n",
        "        (\"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion2.jpg\",\n",
        "         \"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion_n2.jpg\",\n",
        "         \"Part_Whole_Illusion2\"),\n",
        "\n",
        "        (\"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test2.jpg\",\n",
        "         \"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test_n2.jpg\",\n",
        "         \"Margaret_Thatcher2\"),\n",
        "\n",
        "        (\"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test.jpg\",\n",
        "         \"/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test_n.jpg\",\n",
        "         \"Margaret_Thatcher\")\n",
        "    ]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Time Settings (Claim: This is different from the final number of blocks)\n",
        "    time_settings = {\n",
        "        \"0_0_1\": {\"V2\": 0, \"V4\": 0, \"IT\": 1},\n",
        "        \"0_0_2\": {\"V2\": 0, \"V4\": 0, \"IT\": 2},\n",
        "        \"0_0_4\": {\"V2\": 0, \"V4\": 0, \"IT\": 4},\n",
        "        \"0_2_1\": {\"V2\": 0, \"V4\": 2, \"IT\": 1},\n",
        "        \"0_2_2\": {\"V2\": 0, \"V4\": 2, \"IT\": 2},\n",
        "        \"0_2_4\": {\"V2\": 0, \"V4\": 2, \"IT\": 4},\n",
        "        \"2_4_1\": {\"V2\": 2, \"V4\": 4, \"IT\": 1},\n",
        "        \"2_4_2\": {\"V2\": 2, \"V4\": 4, \"IT\": 2},\n",
        "        \"2_4_4\": {\"V2\": 2, \"V4\": 4, \"IT\": 4},\n",
        "        \"5_10_5\": {\"V2\": 5, \"V4\": 10, \"IT\": 5}\n",
        "    }\n",
        "\n",
        "    early_models = [\"0_0_1\", \"0_0_2\", \"0_0_4\", \"0_2_1\", \"0_2_2\", \"0_2_4\"]\n",
        "\n",
        "    def sort_key(name):\n",
        "        return list(map(int, name.split(\"_\")))\n",
        "\n",
        "    hybrid_model = SiameseHybridCORnet(pretrained=False).to(device)\n",
        "    try:\n",
        "        hybrid_model.load_state_dict(torch.load(hybrid_model_path, map_location=device))\n",
        "        print(f\"Loading Hybrid model successfully: {hybrid_model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load Hybrid model: {e}\")\n",
        "        return\n",
        "\n",
        "    pred_dir = os.path.join(OUTPUT_DIR, \"predictions\")\n",
        "    os.makedirs(pred_dir, exist_ok=True)\n",
        "\n",
        "    for img1_path, img2_path, name in image_pairs:\n",
        "        print(f\"\\nPredicting image pair: {name}\")\n",
        "\n",
        "        try:\n",
        "            img1 = transform(Image.open(img1_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "            img2 = transform(Image.open(img2_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "\n",
        "            results = {\n",
        "                'standard': {\"distances\": [], \"probs\": []},\n",
        "                'hybrid': {\"distances\": [], \"probs\": []}\n",
        "            }\n",
        "\n",
        "            # Get hybrid model prediction\n",
        "            hybrid_model.eval()\n",
        "            with torch.no_grad():\n",
        "                out1, out2 = hybrid_model(img1, img2)\n",
        "                dist = F.pairwise_distance(out1, out2).item()\n",
        "                prob = torch.sigmoid(torch.tensor(-10 * dist + 5)).item()\n",
        "\n",
        "            print(f\"Hybrid Model → Distance={dist:.4f}, Probability={prob*100:.2f}%\")\n",
        "            results['hybrid']['distances'].append(dist)\n",
        "            results['hybrid']['probs'].append(prob)\n",
        "\n",
        "            # Get standard models predictions\n",
        "            sorted_keys = sorted(time_settings.keys(), key=sort_key)\n",
        "            for structure_name in sorted_keys:\n",
        "                times_dict = time_settings[structure_name]\n",
        "                model = SiamesePretrainedCORnet(times_dict, pretrained=False, freeze_backbone=False).to(device)\n",
        "                model_path = os.path.join(PREVIOUS_MODELS_PATH, f\"pretrained_model_T{structure_name}.pt\")\n",
        "\n",
        "                if os.path.exists(model_path):\n",
        "                    try:\n",
        "                        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "                    except Exception as e:\n",
        "                        print(f\"Failed to load weights {structure_name}: {e}\")\n",
        "                        continue\n",
        "                else:\n",
        "                    print(f\"Model {structure_name} not found, skipping\")\n",
        "                    continue\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    out1, out2 = model(img1, img2)\n",
        "                    dist = F.pairwise_distance(out1, out2).item()\n",
        "                    prob = torch.sigmoid(torch.tensor(-10 * dist + 5)).item()\n",
        "\n",
        "                print(f\"{structure_name} → Distance={dist:.4f}, Probability={prob*100:.2f}%\")\n",
        "                results['standard']['distances'].append(dist)\n",
        "                results['standard']['probs'].append(prob)\n",
        "\n",
        "            # Plot results\n",
        "            plt.figure(figsize=(16, 6))\n",
        "\n",
        "            # First subplot: All models comparison\n",
        "            plt.subplot(1, 2, 1)\n",
        "            std_probs = [p * 100 for p in results['standard']['probs']]  # Convert to percentage\n",
        "            plt.plot(sorted_keys, std_probs, 'b-o', label=\"Standard Models\", linewidth=1.5, alpha=0.7)\n",
        "            plt.axhline(y=results['hybrid']['probs'][0] * 100, color='g', linestyle='--',\n",
        "                       label=f'Hybrid Model ({results[\"hybrid\"][\"probs\"][0]*100:.1f}%)')\n",
        "\n",
        "            plt.title(f\"Model Comparison - {name}\")\n",
        "            plt.xlabel(\"Time Settings\")\n",
        "            plt.ylabel(\"Probability (%)\")\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.legend()\n",
        "\n",
        "            # Second subplot: Detailed comparison\n",
        "            plt.subplot(1, 2, 2)\n",
        "            compare_models = ['0_0_2', '0_0_4', 'Hybrid']\n",
        "            idx_002 = sorted_keys.index('0_0_2')\n",
        "            idx_004 = sorted_keys.index('0_0_4')\n",
        "\n",
        "            compare_probs = [\n",
        "                results['standard']['probs'][idx_002] * 100,\n",
        "                results['standard']['probs'][idx_004] * 100,\n",
        "                results['hybrid']['probs'][0] * 100\n",
        "            ]\n",
        "\n",
        "            compare_distances = [\n",
        "                results['standard']['distances'][idx_002],\n",
        "                results['standard']['distances'][idx_004],\n",
        "                results['hybrid']['distances'][0]\n",
        "            ]\n",
        "\n",
        "            x = np.arange(len(compare_models))\n",
        "            width = 0.35\n",
        "\n",
        "            ax1 = plt.gca()\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            bars1 = ax1.bar(x - width/2, compare_distances, width, color='blue', alpha=0.7, label='Distance')\n",
        "            ax1.set_ylabel('Distance', color='blue')\n",
        "            ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "            bars2 = ax2.bar(x + width/2, compare_probs, width, color='orange', alpha=0.7, label='Probability')\n",
        "            ax2.set_ylabel('Probability (%)', color='orange')\n",
        "            ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "            for i, d in enumerate(compare_distances):\n",
        "                ax1.text(x[i] - width/2, d + 0.02, f\"{d:.4f}\", ha='center')\n",
        "\n",
        "            for i, p in enumerate(compare_probs):\n",
        "                ax2.text(x[i] + width/2, p + 1, f\"{p:.1f}%\", ha='center')\n",
        "\n",
        "            plt.title(f\"Detailed Comparison - {name}\")\n",
        "            plt.xticks(x, compare_models)\n",
        "            plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "            save_path = os.path.join(OUTPUT_DIR, \"predictions\", f\"{name}_models_comparison.png\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"Saved comparison plot → {save_path}\")\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image pair {name}: {e}\")\n",
        "\n",
        "def generate_hybrid_saliency_maps(dataset, hybrid_model_path, top_percent=0.1, intensity=0.9, num_images=10):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    save_dir = os.path.join(OUTPUT_DIR, \"saliency/hybrid_model\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    hybrid_model = SiameseHybridCORnet(pretrained=False).to(device)\n",
        "    if os.path.exists(hybrid_model_path):\n",
        "        try:\n",
        "            hybrid_model.load_state_dict(torch.load(hybrid_model_path, map_location=device))\n",
        "            print(f\"Loading Hybrid model successfully: {hybrid_model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load Hybrid model: {e}\")\n",
        "            print(\"Using initialized weights\")\n",
        "    else:\n",
        "        print(f\"Cannot find model path: {hybrid_model_path}\")\n",
        "        print(\"Using initialized weights\")\n",
        "\n",
        "    hybrid_model.eval()\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img_tensor, _, _ = dataset[i]\n",
        "        img_tensor = img_tensor.unsqueeze(0).to(device).requires_grad_()\n",
        "\n",
        "        hybrid_model.zero_grad()\n",
        "        out1, _ = hybrid_model(img_tensor, img_tensor)\n",
        "        score = out1.norm()\n",
        "        score.backward()\n",
        "\n",
        "        saliency, _ = torch.max(img_tensor.grad.data.abs(), dim=1)\n",
        "        saliency = saliency[0].detach().cpu().numpy()\n",
        "\n",
        "        flat_saliency = saliency.flatten()\n",
        "        num_top_pixels = int(len(flat_saliency) * top_percent)\n",
        "        threshold = np.partition(flat_saliency, -num_top_pixels)[-num_top_pixels]\n",
        "\n",
        "        img_np = img_tensor[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "        saliency_norm = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-5)\n",
        "        alpha = saliency_norm * intensity\n",
        "\n",
        "        modified_img = img_np.copy()\n",
        "        modified_img[..., 0] += alpha * (1 - modified_img[..., 0])\n",
        "        modified_img[..., 1] *= (1 - alpha)\n",
        "        modified_img[..., 2] *= (1 - alpha)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        axes[0].imshow(img_np)\n",
        "        axes[0].set_title(f\"Original image (Hybrid)\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        axes[1].imshow(modified_img)\n",
        "        axes[1].set_title(f\"Saliency map (Hybrid)\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(save_dir, f\"saliency_{i}.png\")\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Saved saliency map Hybrid sample {i} → {save_path}\")\n",
        "        plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FRuuWQe8ZTO",
        "outputId": "8ee4cd24-e8eb-483c-a1a3-a2dcc8db1db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All directory prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        start_time = datetime.now()\n",
        "        print(f\"Start time: {start_time}\")\n",
        "\n",
        "        ensure_directories()\n",
        "\n",
        "        print(\"\\n[1/3] Start to train Hybrid model\")\n",
        "        try:\n",
        "            hybrid_model = train_hybrid_model(num_epochs=10)\n",
        "            print(\"Model trained!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            print(\"Start following procedure\")\n",
        "\n",
        "        print(\"\\n[2/3] Start evaluation...\")\n",
        "        hybrid_model_path = os.path.join(MODEL_DIR, \"hybrid\", \"hybrid_model.pt\")\n",
        "\n",
        "        if not os.path.exists(hybrid_model_path):\n",
        "            print(f\"Cannot find saved model {hybrid_model_path}\")\n",
        "            best_model_path = os.path.join(MODEL_DIR, \"hybrid\", \"best_model.pt\")\n",
        "            if os.path.exists(best_model_path):\n",
        "                hybrid_model_path = best_model_path\n",
        "                print(f\"Use the best model {best_model_path}\")\n",
        "            else:\n",
        "                print(\"Cannot find best model, failed!\")\n",
        "\n",
        "        print(\"Loading data\")\n",
        "        train_loader, val_loader, train_dataset, val_dataset = get_data_loaders()\n",
        "        print(\"\\n[3/3] Genreralizing model performance result and saliency map\")\n",
        "        try:\n",
        "            predict_models_comparison(hybrid_model_path)\n",
        "            print(\"Finished model predicion\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        try:\n",
        "            generate_hybrid_saliency_maps(\n",
        "                dataset=val_dataset,\n",
        "                hybrid_model_path=hybrid_model_path,\n",
        "                top_percent=0.1,\n",
        "                intensity=0.9,\n",
        "                num_images=10\n",
        "            )\n",
        "            print(\"Saliency maps generalized\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"End time: {end_time}\")\n",
        "        print(f\"Total time: {duration}\")\n",
        "        print(\"Finished Hybrid CORnet experiment\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        experiment_summary = {\n",
        "            \"experiment_name\": \"Hybrid CORnet\",\n",
        "            \"start_time\": str(start_time),\n",
        "            \"end_time\": str(end_time),\n",
        "            \"duration_seconds\": duration.total_seconds(),\n",
        "            \"status\": \"Finished\"\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(MODEL_DIR, \"experiment_summary.json\"), 'w', encoding='utf-8') as f:\n",
        "            json.dump(experiment_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Experiment unfinished\")\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfpRTXeR8dXZ",
        "outputId": "e5e261f0-58d4-4519-a4aa-d920a1c3d687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Start time: 2025-05-12 03:11:53.734531\n",
            "All directory prepared\n",
            "\n",
            "[1/3] Start to train Hybrid model\n",
            "Using devices: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 175MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid model parameters: 52,827,816\n",
            "Trainable parameters: 52,827,816\n",
            "Frozen Proportion: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-50b37aaba1eb>:315: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-7-50b37aaba1eb>:336: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-7-50b37aaba1eb>:358: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train loss: 4.80077, Train acc: 0.50000 | Val loss: 1.01031, Val acc: 0.50133\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch1.pt\n",
            "Find new best (Val acc: 0.50133), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 2/10 | Train loss: 0.62628, Train acc: 0.50001 | Val loss: 0.38029, Val acc: 0.50133\n",
            "Epoch 3/10 | Train loss: 0.30337, Train acc: 0.50105 | Val loss: 0.23784, Val acc: 0.50798\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch3.pt\n",
            "Find new best (Val acc: 0.50798), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 4/10 | Train loss: 0.21557, Train acc: 0.52473 | Val loss: 0.18359, Val acc: 0.59857\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch4.pt\n",
            "Find new best (Val acc: 0.59857), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 5/10 | Train loss: 0.17416, Train acc: 0.63817 | Val loss: 0.14846, Val acc: 0.78690\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch5.pt\n",
            "Find new best (Val acc: 0.78690), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 6/10 | Train loss: 0.14255, Train acc: 0.81043 | Val loss: 0.11738, Val acc: 0.93850\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch6.pt\n",
            "Find new best (Val acc: 0.93850), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 7/10 | Train loss: 0.11344, Train acc: 0.94177 | Val loss: 0.08868, Val acc: 0.99102\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch7.pt\n",
            "Find new best (Val acc: 0.99102), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 8/10 | Train loss: 0.08733, Train acc: 0.99139 | Val loss: 0.06653, Val acc: 0.99934\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch8.pt\n",
            "Find new best (Val acc: 0.99934), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 9/10 | Train loss: 0.06742, Train acc: 0.99919 | Val loss: 0.04965, Val acc: 0.99983\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch9.pt\n",
            "Find new best (Val acc: 0.99983), saved as: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/best_model.pt\n",
            "Epoch 10/10 | Train loss: 0.05259, Train acc: 0.99985 | Val loss: 0.03759, Val acc: 0.99983\n",
            "Check point saves: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/checkpoint_epoch10.pt\n",
            "Training curve saved: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/training_curves/hybrid_training_curves.png\n",
            "Final model is saved in: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/hybrid_model.pt\n",
            "Model trained!\n",
            "\n",
            "[2/3] Start evaluation...\n",
            "Loading data\n",
            "\n",
            "[3/3] Genreralizing model performance result and saliency map\n",
            "Loading Hybrid model successfully: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/hybrid_model.pt\n",
            "\n",
            "Predicting image pair: Part_Whole_Illusion\n",
            "Hybrid Model → Distance=0.4946, Probability=51.35%\n",
            "0_0_1 → Distance=0.2515, Probability=92.31%\n",
            "0_0_2 → Distance=0.1124, Probability=97.97%\n",
            "0_0_4 → Distance=0.1151, Probability=97.91%\n",
            "0_2_1 → Distance=0.6601, Probability=16.79%\n",
            "0_2_2 → Distance=0.2829, Probability=89.76%\n",
            "0_2_4 → Distance=0.2659, Probability=91.22%\n",
            "2_4_1 → Distance=0.4051, Probability=72.09%\n",
            "2_4_2 → Distance=0.2654, Probability=91.26%\n",
            "2_4_4 → Distance=0.0953, Probability=98.28%\n",
            "5_10_5 → Distance=1.0914, Probability=0.27%\n",
            "Saved comparison plot → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/predictions/Part_Whole_Illusion_models_comparison.png\n",
            "\n",
            "Predicting image pair: Part_Whole_Illusion2\n",
            "Hybrid Model → Distance=0.4464, Probability=63.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0_0_1 → Distance=0.1783, Probability=96.15%\n",
            "0_0_2 → Distance=0.2910, Probability=88.99%\n",
            "0_0_4 → Distance=0.0817, Probability=98.50%\n",
            "0_2_1 → Distance=1.2157, Probability=0.08%\n",
            "0_2_2 → Distance=0.1957, Probability=95.45%\n",
            "0_2_4 → Distance=0.9770, Probability=0.84%\n",
            "2_4_1 → Distance=0.1874, Probability=95.79%\n",
            "2_4_2 → Distance=0.3251, Probability=85.18%\n",
            "2_4_4 → Distance=0.1161, Probability=97.89%\n",
            "5_10_5 → Distance=0.9755, Probability=0.85%\n",
            "Saved comparison plot → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/predictions/Part_Whole_Illusion2_models_comparison.png\n",
            "\n",
            "Predicting image pair: Margaret_Thatcher2\n",
            "Hybrid Model → Distance=0.6008, Probability=26.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0_0_1 → Distance=0.7632, Probability=6.71%\n",
            "0_0_2 → Distance=0.5628, Probability=34.80%\n",
            "0_0_4 → Distance=0.4646, Probability=58.76%\n",
            "0_2_1 → Distance=0.4098, Probability=71.13%\n",
            "0_2_2 → Distance=0.6230, Probability=22.62%\n",
            "0_2_4 → Distance=0.7762, Probability=5.94%\n",
            "2_4_1 → Distance=1.2283, Probability=0.07%\n",
            "2_4_2 → Distance=0.2563, Probability=91.96%\n",
            "2_4_4 → Distance=0.0796, Probability=98.53%\n",
            "5_10_5 → Distance=0.5143, Probability=46.44%\n",
            "Saved comparison plot → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/predictions/Margaret_Thatcher2_models_comparison.png\n",
            "\n",
            "Predicting image pair: Margaret_Thatcher\n",
            "Hybrid Model → Distance=1.1785, Probability=0.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0_0_1 → Distance=4.6548, Probability=0.00%\n",
            "0_0_2 → Distance=0.4496, Probability=62.35%\n",
            "0_0_4 → Distance=0.2657, Probability=91.24%\n",
            "0_2_1 → Distance=3.5273, Probability=0.00%\n",
            "0_2_2 → Distance=0.8007, Probability=4.71%\n",
            "0_2_4 → Distance=3.0234, Probability=0.00%\n",
            "2_4_1 → Distance=4.2534, Probability=0.00%\n",
            "2_4_2 → Distance=1.3774, Probability=0.02%\n",
            "2_4_4 → Distance=0.6813, Probability=14.03%\n",
            "5_10_5 → Distance=291.5353, Probability=0.00%\n",
            "Saved comparison plot → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/predictions/Margaret_Thatcher_models_comparison.png\n",
            "Finished model predicion\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Hybrid model successfully: /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/models/hybrid/hybrid_model.pt\n",
            "Saved saliency map Hybrid sample 0 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_0.png\n",
            "Saved saliency map Hybrid sample 1 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_1.png\n",
            "Saved saliency map Hybrid sample 2 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_2.png\n",
            "Saved saliency map Hybrid sample 3 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_3.png\n",
            "Saved saliency map Hybrid sample 4 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_4.png\n",
            "Saved saliency map Hybrid sample 5 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_5.png\n",
            "Saved saliency map Hybrid sample 6 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_6.png\n",
            "Saved saliency map Hybrid sample 7 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_7.png\n",
            "Saved saliency map Hybrid sample 8 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_8.png\n",
            "Saved saliency map Hybrid sample 9 → /content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Fifth Experiment/output/saliency/hybrid_model/saliency_9.png\n",
            "Saliency maps generalized\n",
            "\n",
            "==================================================\n",
            "End time: 2025-05-12 03:46:12.227797\n",
            "Total time: 0:34:18.493266\n",
            "Finished Hybrid CORnet experiment\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('/content/drive/MyDrive/NE240/Holistic Processing /Fifth Experiment')"
      ],
      "metadata": {
        "id": "XxTm0dexDEzV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}