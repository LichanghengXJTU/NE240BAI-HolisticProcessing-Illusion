# -*- coding: utf-8 -*-
"""HP-Cornet-S.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZji7HvHV_L3jzhSHbVZ8-YemvowaC5i
"""

# Download The Pretrained CORnet-S
!pip uninstall -y cornet
!pip install git+https://github.com/dicarlolab/CORnet

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from cornet import cornet_s
from collections import OrderedDict
from cornet.cornet_s import CORblock_S
import json

if os.path.exists("/content/drive"):
    BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Forth Experiment"
    DATA_ROOT = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/LFWCrop_dataset_pytorch"
else:
    BASE_PATH = os.path.dirname(os.path.abspath(__file__))
    DATA_ROOT = os.path.join(os.path.dirname(BASE_PATH), "LFWCrop_dataset_pytorch")

MODEL_DIR = os.path.join(BASE_PATH, "models")
OUTPUT_DIR = os.path.join(BASE_PATH, "output_images")

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Time Settings
time_settings = {
    "0_0_1": {"V2": 0, "V4": 0, "IT": 1},
    "0_0_2": {"V2": 0, "V4": 0, "IT": 2},
    "0_0_4": {"V2": 0, "V4": 0, "IT": 4},
    "0_2_1": {"V2": 0, "V4": 2, "IT": 1},
    "0_2_2": {"V2": 0, "V4": 2, "IT": 2},
    "0_2_4": {"V2": 0, "V4": 2, "IT": 4},
    "2_4_1": {"V2": 2, "V4": 4, "IT": 1},
    "2_4_2": {"V2": 2, "V4": 4, "IT": 2},
    "2_4_4": {"V2": 2, "V4": 4, "IT": 4},
    "5_10_5": {"V2": 5, "V4": 10, "IT": 5}
}

# Point to the file
model_filenames = {
    "0_0_1": "model_T0_0_1.pt",
    "0_0_2": "model_T0_0_2.pt",
    "0_0_4": "model_T0_0_4.pt",
    "0_2_1": "model_T0_2_1.pt",
    "0_2_2": "model_T0_2_2.pt",
    "0_2_4": "model_T0_2_4.pt",
    "2_4_1": "model_T2_4_1.pt",
    "2_4_2": "model_T2_4_2.pt",
    "2_4_4": "model_T2_4_4.pt",
    "5_10_5": "model_T5_10_5.pt"
}

# Prepare to read the image pair
def read_pairs(file_path, label):
    with open(file_path, 'r') as f:
        lines = f.readlines()
    return [(line.strip().split()[0], line.strip().split()[1], label) for line in lines]

def load_all_pairs(list_dir):
    all_pairs = []
    for i in range(1, 11):
        prefix = f"{i:02d}"
        for split in ["train", "test"]:
            same_file = os.path.join(list_dir, f"{prefix}_{split}_same.txt")
            diff_file = os.path.join(list_dir, f"{prefix}_{split}_diff.txt")
            all_pairs += read_pairs(same_file, 1)
            all_pairs += read_pairs(diff_file, 0)
    return all_pairs

class FacePairsDataset(Dataset):
    def __init__(self, pairs, image_dir, transform=None):
        self.pairs = pairs
        self.image_dir = image_dir
        self.transform = transform or transforms.ToTensor()

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        name1, name2, label = self.pairs[idx]
        name1 += ".ppm"
        name2 += ".ppm"
        img1 = Image.open(os.path.join(self.image_dir, name1)).convert("RGB")
        img2 = Image.open(os.path.join(self.image_dir, name2)).convert("RGB")
        return self.transform(img1), self.transform(img2), torch.tensor(label, dtype=torch.float32)

# The CORnet Model
class ChannelAlign(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        if in_channels == out_channels:
            self.proj = nn.Identity()
        else:
            self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)

    def forward(self, x):
        return self.proj(x)

class PretrainedCORnetEmbedding(nn.Module):
    def __init__(self, times_dict, pretrained=True):
        super().__init__()
        self.resnet = models.resnet18(pretrained=pretrained)
        def get_times(region): return times_dict.get(region, 2)
        self.v2_time = get_times('V2')
        self.v4_time = get_times('V4')
        self.it_time = get_times('IT')

        if self.v2_time > 1:
            self.V2_recurrent = CORblock_S(64, 64, times=self.v2_time-1)

        if self.v4_time > 1:
            self.V4_recurrent = CORblock_S(128, 128, times=self.v4_time-1)

        if self.it_time > 1:
            self.IT_recurrent = CORblock_S(512, 512, times=self.it_time-1)

        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()

    def forward(self, x):
        # V1: conv1 + bn1 + relu + maxpool
        x = self.resnet.conv1(x)
        x = self.resnet.bn1(x)
        x = self.resnet.relu(x)
        x = self.resnet.maxpool(x)

        # V2: layer1 (BasicBlock x2)
        x = self.resnet.layer1(x)
        if self.v2_time > 1:
            x = self.V2_recurrent(x)

        # V4: layer2 (BasicBlock x2)
        x = self.resnet.layer2(x)
        if self.v4_time > 1:
            x = self.V4_recurrent(x)

        # IT: layer3 (BasicBlock x2) + layer4 (BasicBlock x2)
        x = self.resnet.layer3(x)
        x = self.resnet.layer4(x)
        if self.it_time > 1:
            x = self.IT_recurrent(x)

        # Pooling and Flatten
        x = self.pool(x)
        x = self.flatten(x)

        return x

class SiamesePretrainedCORnet(nn.Module):
    def __init__(self, times_dict, pretrained=True, freeze_backbone=False):
        super().__init__()
        self.embedding_net = PretrainedCORnetEmbedding(times_dict, pretrained=pretrained)

        if freeze_backbone:
            for param in self.embedding_net.resnet.parameters():
                param.requires_grad = False

            if hasattr(self.embedding_net, 'V2_recurrent'):
                for param in self.embedding_net.V2_recurrent.parameters():
                    param.requires_grad = True

            if hasattr(self.embedding_net, 'V4_recurrent'):
                for param in self.embedding_net.V4_recurrent.parameters():
                    param.requires_grad = True

            if hasattr(self.embedding_net, 'IT_recurrent'):
                for param in self.embedding_net.IT_recurrent.parameters():
                    param.requires_grad = True

            print("已冻结预训练ResNet的权重，只训练时间迭代部分")
        else:
            print("使用预训练权重作为初始化，将训练所有层")

    def forward(self, x1, x2):
        f1 = self.embedding_net(x1)
        f2 = self.embedding_net(x2)
        return f1, f2

class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
        euclidean_distance = F.pairwise_distance(output1, output2)
        loss = (label) * torch.pow(euclidean_distance, 2) + \
               (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)
        return loss.mean()

def compute_accuracy(out1, out2, labels, threshold=0.5):
    distances = F.pairwise_distance(out1, out2)
    preds = (distances < threshold).float()
    correct = (preds == labels).float().sum()
    accuracy = correct / labels.size(0)
    return accuracy.item()

def train_models(time_settings, num_epochs=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    face_dir = os.path.join(DATA_ROOT, "faces")
    list_dir = os.path.join(DATA_ROOT, "lists")
    print(f"Data path: {face_dir}")

    pairs = load_all_pairs(list_dir)
    split_idx = int(0.9 * len(pairs))
    train_pairs = pairs[:split_idx]
    val_pairs = pairs[split_idx:]

    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])

    train_dataset = FacePairsDataset(train_pairs, face_dir, transform=transform)
    val_dataset = FacePairsDataset(val_pairs, face_dir, transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)

    scaler = torch.cuda.amp.GradScaler()

    results = {}

    for t, times_dict in time_settings.items():
        print(f"\n==== Training time setting = {t} ====")

        model = SiameseCORnet(times_dict).to(device)
        criterion = ContrastiveLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=4e-6)

        train_losses, val_losses = [], []
        train_accuracies, val_accuracies = [], []

        for epoch in range(num_epochs):
            model.train()
            running_loss = 0.0
            running_acc = 0.0
            for img1, img2, label in train_loader:
                img1, img2, label = img1.to(device), img2.to(device), label.to(device)
                optimizer.zero_grad()

                with torch.cuda.amp.autocast():
                    out1, out2 = model(img1, img2)
                    loss = criterion(out1, out2, label)

                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()

                acc = compute_accuracy(out1, out2, label)
                running_loss += loss.item()
                running_acc += acc

            train_loss = running_loss / len(train_loader)
            train_acc = running_acc / len(train_loader)

            model.eval()
            running_loss = 0.0
            running_acc = 0.0
            with torch.no_grad():
                for img1, img2, label in val_loader:
                    img1, img2, label = img1.to(device), img2.to(device), label.to(device)

                    with torch.cuda.amp.autocast():
                        out1, out2 = model(img1, img2)
                        loss = criterion(out1, out2, label)

                    acc = compute_accuracy(out1, out2, label)
                    running_loss += loss.item()
                    running_acc += acc

            val_loss = running_loss / len(val_loader)
            val_acc = running_acc / len(val_loader)

            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_accuracies.append(train_acc)
            val_accuracies.append(val_acc)

            print(f"[T={t}] Epoch {epoch+1}/{num_epochs} | Training loss: {train_loss:.5f}, Training accuracy: {train_acc:.5f} | Validation loss: {val_loss:.5f}, Validation accuracy: {val_acc:.5f}")

            epoch_results = {
                'train_losses': train_losses,
                'val_losses': val_losses,
                'train_accs': train_accuracies,
                'val_accs': val_accuracies,
                'current_epoch': epoch + 1
            }

            epoch_data_dir = os.path.join(MODEL_DIR, "standard", "epoch_data", t)
            os.makedirs(epoch_data_dir, exist_ok=True)

            with open(os.path.join(epoch_data_dir, f"epoch_{epoch+1}_data.json"), 'w') as f:
                json.dump(epoch_results, f, indent=2)

            print(f"Epoch {epoch+1} data saved for time setting {t}")

            if (epoch + 1) % 2 == 0:
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': train_loss,
                }
                checkpoint_path = os.path.join(MODEL_DIR, f"checkpoint_T{t}_epoch{epoch+1}.pt")
                torch.save(checkpoint, checkpoint_path)
                print(f"Checkpoint saved: {checkpoint_path}")
# Save the model
        save_path = os.path.join(MODEL_DIR, f"model_T{t}.pt")
        torch.save(model.state_dict(), save_path)
        print(f"Model {t} saved to {save_path}")

        results[t] = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accuracies,
            'val_accs': val_accuracies
        }

    with open(os.path.join(BASE_PATH, 'training_results.json'), 'w') as f:
        json.dump(results, f)

    return results

def predict_pair_over_time(img1_path, img2_path, time_settings, model_filenames, threshold=0.5, alpha=10, beta=5, use_pretrained=False, freeze_backbone=False):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])

    img1 = transform(Image.open(img1_path).convert("RGB")).unsqueeze(0).to(device)
    img2 = transform(Image.open(img2_path).convert("RGB")).unsqueeze(0).to(device)

    distances = []
    probs = []

    def sort_key(name):
        return list(map(int, name.split("_")))

    sorted_keys = sorted(time_settings.keys(), key=sort_key)

    for structure_name in sorted_keys:
        times_dict = time_settings[structure_name]
        if use_pretrained:
            model = SiamesePretrainedCORnet(times_dict, pretrained=True, freeze_backbone=False).to(device)
            print(f"Using pretrained model for time setting {structure_name}")

            mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
            pretrained_model_dir = os.path.join(MODEL_DIR, "pretrained", mode_dir)
            pretrained_model_path = os.path.join(pretrained_model_dir, f"pretrained_model_T{structure_name}.pt")

            if os.path.exists(pretrained_model_path):
                try:
                    model.load_state_dict(torch.load(pretrained_model_path, map_location=device))
                    print(f"Loaded pretrained model weights from {pretrained_model_path}")
                except Exception as e:
                    print(f"Could not load pretrained weights for {structure_name}: {e}")
                    print("Using base pretrained ResNet weights")
            else:
                print(f"No trained weights found for {structure_name} in {pretrained_model_dir}, using base pretrained ResNet weights")
        else:
            model = SiameseCORnet(times_dict).to(device)

            model_file = model_filenames[structure_name]
            model_path = os.path.join(MODEL_DIR, model_file)

            if os.path.exists(model_path):
                try:
                    model.load_state_dict(torch.load(model_path, map_location=device))
                    print(f"Loaded model weights from {model_path}")
                except Exception as e:
                    print(f"Could not load weights for {structure_name}: {e}")
            else:
                print(f"No saved weights for {structure_name}")

        model.eval()

        with torch.no_grad():
            emb1, emb2 = model(img1, img2)
            dist = F.pairwise_distance(emb1, emb2).item()
            prob = torch.sigmoid(torch.tensor(-alpha * dist + beta)).item()
            distances.append(dist)
            probs.append(prob)

        print(f"{structure_name} → Distance={dist:.4f}, Same probability={prob:.4f}")

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(sorted_keys, distances, 'b-o', label="Distance", linewidth=1.5, alpha=0.7)
    for i, key in enumerate(sorted_keys):
        if key == "1_2_2":
            plt.plot(key, distances[i], 'ro', markersize=8, label=f'{key} (emphasized)', zorder=10)
    plt.title("Time setting and Euclidean distance")
    plt.xlabel(r"Time setting ($t_1\_t_2\_t_3$)")
    plt.ylabel("Distance")
    plt.xticks(rotation=45)
    plt.grid()

    plt.subplot(1, 2, 2)
    plt.plot(sorted_keys, [p * 100 for p in probs], 'b-o', label="Probability", linewidth=1.5, alpha=0.7)
    for i, key in enumerate(sorted_keys):
        if key == "1_2_2":
            plt.plot(key, [p * 100 for p in probs][i], 'ro', markersize=8, label=f'{key} (emphasized)', zorder=10)
    plt.title("Time setting and prediction probability")
    plt.xlabel(r"Time setting ($t_1\_t_2\_t_3$)")
    plt.ylabel("Probability(%)")
    plt.xticks(rotation=45)
    plt.grid()

    plt.tight_layout()
    save_dir = os.path.join(OUTPUT_DIR, "predictions")
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "prediction_comparison.png")
    plt.savefig(save_path)
    print(f"Saved prediction comparison → {save_path}")
    plt.show()

    results = {
        'sorted_keys': sorted_keys,
        'distances': distances,
        'probabilities': probs
    }
    with open(os.path.join(save_dir, 'prediction_results.json'), 'w') as f:
        json.dump(results, f, indent=2)

def generate_saliency_over_time_batch(dataset, time_settings, top_percent=0.1, intensity=0.9, num_images=10, use_pretrained=False, freeze_backbone=False):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    for t, times_dict in time_settings.items():
        mode_str = ""
        if use_pretrained:
            mode_str = "_pretrained_" + ("frozen" if freeze_backbone else "full")

        save_dir = os.path.join(OUTPUT_DIR, f"saliency/time={t}{mode_str}")
        os.makedirs(save_dir, exist_ok=True)

        if use_pretrained:
            model = SiamesePretrainedCORnet(times_dict, pretrained=True, freeze_backbone=False).to(device)
            print(f"Using pretrained model for saliency time setting {t}")

            mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
            pretrained_model_dir = os.path.join(MODEL_DIR, "pretrained", mode_dir)
            pretrained_model_path = os.path.join(pretrained_model_dir, f"pretrained_model_T{t}.pt")

            if os.path.exists(pretrained_model_path):
                try:
                    model.load_state_dict(torch.load(pretrained_model_path, map_location=device))
                    print(f"Loaded pretrained model weights from {pretrained_model_path}")
                except Exception as e:
                    print(f"Could not load pretrained weights for {t}: {e}")
                    print("Using base pretrained ResNet weights")
            else:
                print(f"No trained weights found for {t} in {pretrained_model_dir}, using base pretrained ResNet weights")
        else:
            model = SiameseCORnet(times_dict).to(device)

            model_path = os.path.join(MODEL_DIR, f"model_T{t}.pt")
            if os.path.exists(model_path):
                try:
                    model.load_state_dict(torch.load(model_path, map_location=device))
                    print(f"Loaded model weights from {model_path}")
                except Exception as e:
                    print(f"Could not load weights for {t}: {e}")
            else:
                print(f"No saved weights for {t}")

        model.eval()

        for i in range(num_images):
            img_tensor, _, _ = dataset[i]
            img_tensor = img_tensor.unsqueeze(0).to(device).requires_grad_()

            model.zero_grad()
            out1, _ = model(img_tensor, img_tensor)
            score = out1.norm()
            score.backward()

            saliency, _ = torch.max(img_tensor.grad.data.abs(), dim=1)
            saliency = saliency[0].detach().cpu().numpy()

            flat_saliency = saliency.flatten()
            num_top_pixels = int(len(flat_saliency) * top_percent)
            threshold = np.partition(flat_saliency, -num_top_pixels)[-num_top_pixels]

            img_np = img_tensor[0].detach().cpu().permute(1, 2, 0).numpy()
            img_np = np.clip(img_np, 0, 1)
            saliency_norm = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-5)
            alpha = saliency_norm * intensity

            modified_img = img_np.copy()
            modified_img[..., 0] += alpha * (1 - modified_img[..., 0])
            modified_img[..., 1] *= (1 - alpha)
            modified_img[..., 2] *= (1 - alpha)

            fig, axes = plt.subplots(1, 2, figsize=(8, 4))
            axes[0].imshow(img_np)
            axes[0].set_title(f"Original image (T={t})")
            axes[0].axis("off")

            axes[1].imshow(modified_img)
            axes[1].set_title(f"Saliency map (T={t})")
            axes[1].axis("off")

            plt.tight_layout()
            save_path = os.path.join(save_dir, f"saliency_{i}.png")
            plt.savefig(save_path)
            print(f"Saved saliency map T={t} sample {i} → {save_path}")
            plt.close()


# Prediction
def compare_models_performance(val_dataset, time_settings, threshold=0.5, alpha=10, beta=5, freeze_backbone=False):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)

    results = {
        'standard': {},
        'pretrained': {}
    }

    for t, times_dict in time_settings.items():
        print(f"\n==== Comparing models for time setting {t} ====")

        standard_model = SiameseCORnet(times_dict).to(device)
        standard_model_path = os.path.join(MODEL_DIR, f"model_T{t}.pt")
        standard_accuracy = 0

        if os.path.exists(standard_model_path):
            try:
                standard_model.load_state_dict(torch.load(standard_model_path, map_location=device))
                print(f"Loaded standard model from {standard_model_path}")

                standard_model.eval()
                running_acc = 0.0
                with torch.no_grad():
                    for img1, img2, label in val_loader:
                        img1, img2, label = img1.to(device), img2.to(device), label.to(device)
                        out1, out2 = standard_model(img1, img2)
                        acc = compute_accuracy(out1, out2, label, threshold=threshold)
                        running_acc += acc

                standard_accuracy = running_acc / len(val_loader)
                print(f"Standard model accuracy: {standard_accuracy:.5f}")
            except Exception as e:
                print(f"Could not load standard model: {e}")
        else:
            print(f"No standard model found for time setting {t}")

        pretrained_model = SiamesePretrainedCORnet(times_dict, pretrained=True, freeze_backbone=freeze_backbone).to(device)
        mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
        pretrained_model_path = os.path.join(MODEL_DIR, "pretrained", mode_dir, f"pretrained_model_T{t}.pt")
        pretrained_accuracy = 0

        if os.path.exists(pretrained_model_path):
            try:
                pretrained_model.load_state_dict(torch.load(pretrained_model_path, map_location=device))
                print(f"Loaded pretrained model from {pretrained_model_path}")

                pretrained_model.eval()
                running_acc = 0.0
                with torch.no_grad():
                    for img1, img2, label in val_loader:
                        img1, img2, label = img1.to(device), img2.to(device), label.to(device)
                        out1, out2 = pretrained_model(img1, img2)
                        acc = compute_accuracy(out1, out2, label, threshold=threshold)
                        running_acc += acc

                pretrained_accuracy = running_acc / len(val_loader)
                print(f"Pretrained model accuracy: {pretrained_accuracy:.5f}")
            except Exception as e:
                print(f"Could not load pretrained model: {e}")
        else:
            print(f"No fine-tuned pretrained model found for time setting {t}, using base pretrained model")

            pretrained_model.eval()
            running_acc = 0.0
            with torch.no_grad():
                for img1, img2, label in val_loader:
                    img1, img2, label = img1.to(device), img2.to(device), label.to(device)
                    out1, out2 = pretrained_model(img1, img2)
                    acc = compute_accuracy(out1, out2, label, threshold=threshold)
                    running_acc += acc

            pretrained_accuracy = running_acc / len(val_loader)
            print(f"Base pretrained model accuracy: {pretrained_accuracy:.5f}")

        results['standard'][t] = standard_accuracy
        results['pretrained'][t] = pretrained_accuracy

    plt.figure(figsize=(12, 6))

    sorted_keys = sorted(time_settings.keys(), key=lambda x: list(map(int, x.split('_'))))

    std_accs = [results['standard'].get(k, 0) * 100 for k in sorted_keys]
    pre_accs = [results['pretrained'].get(k, 0) * 100 for k in sorted_keys]

    x = np.arange(len(sorted_keys))
    width = 0.35

    plt.bar(x - width/2, std_accs, width, label='Standard CORnet')
    mode_label = f'Pretrained CORnet ({"Frozen Backbone" if freeze_backbone else "Full Finetune"})'
    plt.bar(x + width/2, pre_accs, width, label=mode_label)

    plt.xlabel('Time settings')
    plt.ylabel('Accuracy (%)')
    plt.title('Comparison of model performance')
    plt.xticks(x, sorted_keys, rotation=45)
    plt.legend()
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    mode_str = "frozen" if freeze_backbone else "full"
    comparison_dir = os.path.join(OUTPUT_DIR, "comparisons")
    os.makedirs(comparison_dir, exist_ok=True)
    plt.savefig(os.path.join(comparison_dir, f"model_comparison_{mode_str}.png"))

    with open(os.path.join(comparison_dir, f"model_comparison_results_{mode_str}.json"), "w") as f:
        json.dump(results, f, indent=2)

    return results

def plot_training_curves(time_settings, use_pretrained=True, freeze_backbone=False):
    print("\nGenerating training process curves...")

    if use_pretrained:
        mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
        model_type = f"pretrained_{mode_dir}"
    else:
        model_type = "standard"

    if use_pretrained:
        base_dir = os.path.join(MODEL_DIR, "pretrained", mode_dir, "epoch_data")
    else:
        base_dir = os.path.join(MODEL_DIR, "standard", "epoch_data")

    if not os.path.exists(base_dir):
        print(f"训练数据目录 {base_dir} 不存在，无法生成训练曲线")
        return

    curves_dir = os.path.join(OUTPUT_DIR, f"{model_type}_training_curves")
    os.makedirs(curves_dir, exist_ok=True)

    for t in time_settings.keys():
        t_dir = os.path.join(base_dir, t)

        if not os.path.exists(t_dir):
            print(f"时间设置 {t} 的训练数据不存在，跳过")
            continue

        epoch_files = [f for f in os.listdir(t_dir) if f.startswith("epoch_") and f.endswith("_data.json")]
        if not epoch_files:
            print(f"时间设置 {t} 没有找到训练数据文件，跳过")
            continue

        epoch_files.sort(key=lambda x: int(x.split("_")[1]))
        latest_file = epoch_files[-1]

        with open(os.path.join(t_dir, latest_file), 'r') as f:
            data = json.load(f)

        epochs = list(range(1, data['current_epoch'] + 1))
        train_losses = data['train_losses']
        val_losses = data['val_losses']
        train_accs = data['train_accs']
        val_accs = data['val_accs']

        fig, axes = plt.subplots(2, 1, figsize=(10, 12))

        axes[0].plot(epochs, train_losses, 'b-', label='Training Loss')
        axes[0].plot(epochs, val_losses, 'r-', label='Validation Loss')
        axes[0].set_title(f'Loss Curves for Time Setting {t} ({model_type})')
        axes[0].set_xlabel('Epochs')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].grid(True)

        axes[1].plot(epochs, [acc * 100 for acc in train_accs], 'b-', label='Training Accuracy')
        axes[1].plot(epochs, [acc * 100 for acc in val_accs], 'r-', label='Validation Accuracy')
        axes[1].set_title(f'Accuracy Curves for Time Setting {t} ({model_type})')
        axes[1].set_xlabel('Epochs')
        axes[1].set_ylabel('Accuracy (%)')
        axes[1].legend()
        axes[1].grid(True)

        plt.tight_layout()
        save_path = os.path.join(curves_dir, f"{t}_training_curves.png")
        plt.savefig(save_path)
        plt.close()

        print(f"Training curves for time setting {t} saved to {save_path}")

    val_accs_final = {}
    for t in time_settings.keys():
        t_dir = os.path.join(base_dir, t)
        if not os.path.exists(t_dir):
            continue

        epoch_files = [f for f in os.listdir(t_dir) if f.startswith("epoch_") and f.endswith("_data.json")]
        if not epoch_files:
            continue

        epoch_files.sort(key=lambda x: int(x.split("_")[1]))
        latest_file = epoch_files[-1]

        with open(os.path.join(t_dir, latest_file), 'r') as f:
            data = json.load(f)

        val_accs_final[t] = data['val_accs'][-1]

    if val_accs_final:
        sorted_items = sorted(val_accs_final.items(), key=lambda x: list(map(int, x[0].split('_'))))
        sorted_keys = [item[0] for item in sorted_items]
        sorted_values = [item[1] * 100 for item in sorted_items]

        plt.figure(figsize=(12, 6))
        bars = plt.bar(sorted_keys, sorted_values)

        for bar, value in zip(bars, sorted_values):
            plt.text(bar.get_x() + bar.get_width()/2, value + 1,
                    f'{value:.2f}%', ha='center', va='bottom', rotation=0)

        plt.title(f'Final Validation Accuracy Comparison ({model_type})')
        plt.xlabel('Time Settings')
        plt.ylabel('Validation Accuracy (%)')
        plt.xticks(rotation=45)
        plt.ylim(0, 105)
        plt.grid(axis='y', linestyle='--', alpha=0.7)

        plt.tight_layout()
        summary_path = os.path.join(curves_dir, "final_accuracy_comparison.png")
        plt.savefig(summary_path)
        plt.close()

        print(f"Final accuracy comparison saved to {summary_path}")

def main():
    """主函数：一次性运行整个实验流程"""
    print("Begin the experiment of CORnet...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    face_dir = os.path.join(DATA_ROOT, "faces")
    list_dir = os.path.join(DATA_ROOT, "lists")

    pairs = load_all_pairs(list_dir)
    split_idx = int(0.9 * len(pairs))
    train_pairs = pairs[:split_idx]
    val_pairs = pairs[split_idx:]

    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])

    train_dataset = FacePairsDataset(train_pairs, face_dir, transform=transform)
    val_dataset = FacePairsDataset(val_pairs, face_dir, transform=transform)

    should_train = True
    use_pretrained = True
    freeze_backbone = False

    if should_train:
        print("Start training models...")
        if use_pretrained:
            print(f"使用预训练模型作为初始参数训练所有时间设置")

            results = {}

            for t, times_dict in time_settings.items():
                print(f"\n==== Training pretrained model for time setting = {t} ====")

                model = SiamesePretrainedCORnet(times_dict, pretrained=True, freeze_backbone=freeze_backbone).to(device)

                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                print(f"总参数数量: {total_params:,}")
                print(f"可训练参数数量: {trainable_params:,}")
                print(f"冻结参数比例: {100 * (total_params - trainable_params) / total_params:.2f}%")

                criterion = ContrastiveLoss()

                optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

                train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)
                val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)

                scaler = torch.cuda.amp.GradScaler()

                train_losses, val_losses = [], []
                train_accuracies, val_accuracies = [], []

                num_epochs = 10

                for epoch in range(num_epochs):
                    model.train()
                    running_loss = 0.0
                    running_acc = 0.0
                    for img1, img2, label in train_loader:
                        img1, img2, label = img1.to(device), img2.to(device), label.to(device)
                        optimizer.zero_grad()

                        with torch.cuda.amp.autocast():
                            out1, out2 = model(img1, img2)
                            loss = criterion(out1, out2, label)

                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()

                        acc = compute_accuracy(out1, out2, label)
                        running_loss += loss.item()
                        running_acc += acc

                    train_loss = running_loss / len(train_loader)
                    train_acc = running_acc / len(train_loader)

                    model.eval()
                    running_loss = 0.0
                    running_acc = 0.0
                    with torch.no_grad():
                        for img1, img2, label in val_loader:
                            img1, img2, label = img1.to(device), img2.to(device), label.to(device)

                            with torch.cuda.amp.autocast():
                                out1, out2 = model(img1, img2)
                                loss = criterion(out1, out2, label)

                            acc = compute_accuracy(out1, out2, label)
                            running_loss += loss.item()
                            running_acc += acc

                    val_loss = running_loss / len(val_loader)
                    val_acc = running_acc / len(val_loader)

                    train_losses.append(train_loss)
                    val_losses.append(val_loss)
                    train_accuracies.append(train_acc)
                    val_accuracies.append(val_acc)

                    print(f"[Pretrained T={t}] Epoch {epoch+1}/{num_epochs} | Training loss: {train_loss:.5f}, Training accuracy: {train_acc:.5f} | Validation loss: {val_loss:.5f}, Validation accuracy: {val_acc:.5f}")

                    epoch_results = {
                        'train_losses': train_losses,
                        'val_losses': val_losses,
                        'train_accs': train_accuracies,
                        'val_accs': val_accuracies,
                        'current_epoch': epoch + 1
                    }

                    mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
                    epoch_data_dir = os.path.join(MODEL_DIR, "pretrained", mode_dir, "epoch_data", t)
                    os.makedirs(epoch_data_dir, exist_ok=True)

                    with open(os.path.join(epoch_data_dir, f"epoch_{epoch+1}_data.json"), 'w') as f:
                        json.dump(epoch_results, f, indent=2)

                    print(f"Epoch {epoch+1} data saved for time setting {t}")

                mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
                pretrained_model_dir = os.path.join(MODEL_DIR, "pretrained", mode_dir)
                os.makedirs(pretrained_model_dir, exist_ok=True)
                save_path = os.path.join(pretrained_model_dir, f"pretrained_model_T{t}.pt")
                torch.save(model.state_dict(), save_path)
                print(f"Pretrained model {t} saved to {save_path}")

                results[t] = {
                    'train_losses': train_losses,
                    'val_losses': val_losses,
                    'train_accs': train_accuracies,
                    'val_accs': val_accuracies
                }

            mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
            with open(os.path.join(BASE_PATH, f'pretrained_{mode_dir}_training_results.json'), 'w') as f:
                json.dump(results, f, indent=2)

            print("所有时间设置的训练结果已保存")

        else:
            train_models(time_settings, num_epochs=10)
    else:
        print("Skip the training stage, use the existing models")
        source_model_dir = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Third Experiment/models"
        if not all(os.path.exists(os.path.join(MODEL_DIR, model_filenames[t])) for t in time_settings):
            print("Copying models from the third experiment...")
            for t, filename in model_filenames.items():
                source_file = os.path.join(source_model_dir, filename.replace(f"model_T{t}.pt", f"model_T{t.replace('_', '')}.pt"))
                if os.path.exists(source_file):
                    target_file = os.path.join(MODEL_DIR, f"model_T{t}.pt")
                    os.makedirs(os.path.dirname(target_file), exist_ok=True)
                    if not os.path.exists(target_file):
                        print(f"Copying {source_file} to {target_file}")
                        import shutil
                        shutil.copy(source_file, target_file)

    if os.path.exists("/content/drive"):
        mt_normal = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test.jpg"
        mt_inverted = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test_n.jpg"
    else:
        mt_normal = os.path.join(BASE_PATH, "test_images/Margaret Thatcher_test.jpg")
        mt_inverted = os.path.join(BASE_PATH, "test_images/Margaret Thatcher_test_n.jpg")
        test_dir = os.path.join(BASE_PATH, "test_images")
        os.makedirs(test_dir, exist_ok=True)
        if not (os.path.exists(mt_normal) and os.path.exists(mt_inverted)):
            print(f"测试图像不存在，请将测试图像放到 {test_dir} 目录下")
            print(f"或者修改代码指定正确的图像路径")
            if os.path.exists(DATA_ROOT):
                print("使用数据集中的图像作为替代...")
                face_dir = os.path.join(DATA_ROOT, "faces")
                image_files = [f for f in os.listdir(face_dir) if f.endswith('.ppm')][:2]
                if len(image_files) >= 2:
                    mt_normal = os.path.join(face_dir, image_files[0])
                    mt_inverted = os.path.join(face_dir, image_files[1])
                    print(f"使用 {mt_normal} 和 {mt_inverted} 作为测试图像")

    print("\nStart predicting Margaret Thatcher effect image pairs...")
    predict_pair_over_time(mt_normal, mt_inverted, time_settings, model_filenames, use_pretrained=use_pretrained, freeze_backbone=freeze_backbone)

    print("\nStart generating saliency maps...")
    if use_pretrained:
        print("Generating saliency maps for all pretrained models...")
        generate_saliency_over_time_batch(val_dataset, time_settings, num_images=10, use_pretrained=True, freeze_backbone=freeze_backbone)
    else:
        print("Generating saliency maps for all standard models...")
        generate_saliency_over_time_batch(val_dataset, time_settings, num_images=10, use_pretrained=False, freeze_backbone=False)

    compare_models_performance(val_dataset, time_settings, freeze_backbone=freeze_backbone)

    if should_train:
        if use_pretrained:
            plot_training_curves(time_settings, use_pretrained=True, freeze_backbone=freeze_backbone)
        else:
            plot_training_curves(time_settings, use_pretrained=False, freeze_backbone=False)

    print("Experiment completed!")

if __name__ == "__main__":
    main()

import os
import torch
from torchvision import transforms

if os.path.exists("/content/drive"):
    BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Forth Experiment"
    DATA_ROOT = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/LFWCrop_dataset_pytorch"
else:
    BASE_PATH = os.path.dirname(os.path.abspath(__file__))
    DATA_ROOT = os.path.join(os.path.dirname(BASE_PATH), "LFWCrop_dataset_pytorch")

print("加载数据集...")
face_dir = os.path.join(DATA_ROOT, "faces")
list_dir = os.path.join(DATA_ROOT, "lists")

pairs = load_all_pairs(list_dir)
split_idx = int(0.9 * len(pairs))
val_pairs = pairs[split_idx:]

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

val_dataset = FacePairsDataset(val_pairs, face_dir, transform=transform)
print(f"验证集大小: {len(val_dataset)}张图像对")

print("\n为所有时间设置生成10张显著图...")
use_pretrained = True
freeze_backbone = False

generate_saliency_over_time_batch(
    val_dataset,
    time_settings,
    num_images=10,
    top_percent=0.1,
    intensity=0.9,
    use_pretrained=use_pretrained,
    freeze_backbone=freeze_backbone
)

print("显著图生成完成！")

import os
import json
import matplotlib.pyplot as plt
import numpy as np

if os.path.exists("/content/drive"):
    BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /Forth Experiment"
else:
    BASE_PATH = os.path.dirname(os.path.abspath(__file__))

MODEL_DIR = os.path.join(BASE_PATH, "models")
OUTPUT_DIR = os.path.join(BASE_PATH, "output_images")

time_settings = [
    "0_0_1", "0_0_2", "0_0_4", "0_2_1", "0_2_2",
    "0_2_4", "2_4_1", "2_4_2", "2_4_4", "5_10_5"
]

use_pretrained = True
freeze_backbone = False

if use_pretrained:
    mode_dir = "frozen_backbone" if freeze_backbone else "full_finetune"
    base_dir = os.path.join(MODEL_DIR, "pretrained", mode_dir, "epoch_data")
    title_prefix = "Pretrained CORnet "
    title_suffix = ""
else:
    base_dir = os.path.join(MODEL_DIR, "standard", "epoch_data")
    title_prefix = "Standard CORnet"
    title_suffix = ""

plt.figure(figsize=(20, 10))

colors = plt.cm.tab10(np.linspace(0, 1, len(time_settings)))

plt.subplot(1, 2, 1)

all_losses = []

for i, t in enumerate(time_settings):
    t_dir = os.path.join(base_dir, t)

    if not os.path.exists(t_dir):
        print(f"警告: 时间设置 {t} 的训练数据不存在，跳过")
        continue

    epoch_files = [f for f in os.listdir(t_dir) if f.startswith("epoch_") and f.endswith("_data.json")]
    if not epoch_files:
        print(f"警告: 时间设置 {t} 没有找到训练数据文件，跳过")
        continue

    epoch_files.sort(key=lambda x: int(x.split("_")[1]))
    latest_file = epoch_files[-1]

    with open(os.path.join(t_dir, latest_file), 'r') as f:
        data = json.load(f)

    epochs = list(range(1, data['current_epoch'] + 1))
    train_losses = data['train_losses']
    val_losses = data['val_losses']

    if t != "5_10_5":
        all_losses.extend(val_losses)

    line, = plt.plot(epochs, val_losses, '-', color=colors[i], linewidth=2, label=f'T={t}')
    line, = plt.plot(epochs, train_losses, '--', color=colors[i], linewidth=2, label=f'T={t}')
    # Special processing for 5_10_5
    if t == "5_10_5":
        plt.annotate(f'5_10_5 max={max(val_losses):.2f}',
                     xy=(epochs[-1], val_losses[-1]),
                     xytext=(epochs[-1], val_losses[-1] * 0.8),
                     arrowprops=dict(facecolor='black', shrink=0.05),
                     horizontalalignment='right')
        plt.annotate(f'5_10_5 max={max(train_losses):.2f}',
                     xy=(epochs[-1], train_losses[-1]),
                     xytext=(epochs[-1], train_losses[-1] * 0.8),
                     arrowprops=dict(facecolor='black', shrink=0.05),
                     horizontalalignment='right')

if all_losses:
    y_max = max(all_losses) * 1.2
    plt.ylim(0, y_max)

    plt.text(0.02, 0.98, "Y-axis truncated\n5_10_5 values exceed range",
             transform=plt.gca().transAxes,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.title(f'Loss Comparison - {title_prefix}{title_suffix}', fontsize=16)
plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Loss', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.7)

plt.subplot(1, 2, 2)
for i, t in enumerate(time_settings):
    t_dir = os.path.join(base_dir, t)

    if not os.path.exists(t_dir) or not os.listdir(t_dir):
        continue

    epoch_files = [f for f in os.listdir(t_dir) if f.startswith("epoch_") and f.endswith("_data.json")]
    if not epoch_files:
        continue

    epoch_files.sort(key=lambda x: int(x.split("_")[1]))
    latest_file = epoch_files[-1]

    with open(os.path.join(t_dir, latest_file), 'r') as f:
        data = json.load(f)

    epochs = list(range(1, data['current_epoch'] + 1))
    val_accs = data['val_accs']
    train_accs = data['train_accs']

    plt.plot(epochs, [acc * 100 for acc in train_accs], '-', color=colors[i], linewidth=2, label=f'T={t}')
    plt.plot(epochs, [acc * 100 for acc in val_accs], '--', color=colors[i], linewidth=2, label=f'T={t}')


plt.title(f'Accuracy Comparison - {title_prefix}{title_suffix}', fontsize=16)
plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Accuracy (%)', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.7)
plt.suptitle(f'Training Curves for All Time Settings - {title_prefix}{title_suffix}', fontsize=20)

plt.tight_layout(rect=[0, 0, 1, 0.90])

handles, labels = plt.gca().get_legend_handles_labels()
plt.gcf().legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=5, fontsize=12)

save_path = os.path.join(OUTPUT_DIR, f"all_models_{'pretrained' if use_pretrained else 'standard'}_{'frozen' if use_pretrained and freeze_backbone else 'full'}_comparison.png")
plt.savefig(save_path, dpi=300, bbox_inches='tight')
print(f"已保存合并图表到: {save_path}")

plt.show()

import os
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
from torchvision import transforms

save_dir = os.path.join(OUTPUT_DIR, "predictions")
os.makedirs(save_dir, exist_ok=True)

def predict_all_images():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    image_pairs = [
        ("/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion.jpg",
         "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion_n.jpg",
         "Part_Whole_Illusion"),

        ("/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion2.jpg",
         "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Part_Whole_Illusion_n2.jpg",
         "Part_Whole_Illusion2"),

        ("/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test2.jpg",
         "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test_n2.jpg",
         "Margaret_Thatcher2"),
        (
            "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test.jpg",
            "/content/drive/MyDrive/Colab Notebooks/NE240/Holistic Processing /First Experiment/Margaret Thatcher_test_n.jpg",
            "Margaret_Thatcher"
        )
    ]

    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])

    def sort_key(name):
        return list(map(int, name.split("_")))

    early_models = ["0_0_1", "0_0_2", "0_0_4", "0_2_1", "0_2_2", "0_2_4"]

    for img1_path, img2_path, name in image_pairs:
        print(f"\n预测图片对: {name}")
        try:
            img1 = transform(Image.open(img1_path).convert("RGB")).unsqueeze(0).to(device)
            img2 = transform(Image.open(img2_path).convert("RGB")).unsqueeze(0).to(device)

            distances = []
            probs = []

            sorted_keys = sorted(time_settings.keys(), key=sort_key)

            for structure_name in sorted_keys:
                times_dict = time_settings[structure_name]

                model = SiamesePretrainedCORnet(times_dict, pretrained=True, freeze_backbone=False).to(device)

                pretrained_model_dir = os.path.join(MODEL_DIR, "pretrained", "full_finetune")
                pretrained_model_path = os.path.join(pretrained_model_dir, f"pretrained_model_T{structure_name}.pt")

                if os.path.exists(pretrained_model_path):
                    try:
                        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))
                        print(f"加载模型权重: {pretrained_model_path}")
                    except Exception as e:
                        print(f"无法加载权重 {structure_name}: {e}")
                else:
                    print(f"未找到模型 {structure_name}, 使用基础预训练权重")

                model.eval()

                with torch.no_grad():
                    emb1, emb2 = model(img1, img2)
                    dist = F.pairwise_distance(emb1, emb2).item()
                    prob = torch.sigmoid(torch.tensor(-10 * dist + 5)).item()
                    distances.append(dist)
                    probs.append(prob)

                print(f"{structure_name} → 距离={dist:.4f}, 相同概率={prob:.4f}")

            overall_max_idx = np.argmax(probs)

            early_indices = [i for i, key in enumerate(sorted_keys) if key in early_models]
            early_max_idx = early_indices[np.argmax([probs[i] for i in early_indices])]

            plt.figure(figsize=(12, 5))

            plt.subplot(1, 2, 1)
            plt.plot(sorted_keys, distances, 'b-o', label="Distance", linewidth=1.5, alpha=0.7)
            plt.title(f"Euclidean distances - {name}")
            plt.xlabel(r"Time setting ($t_1\_t_2\_t_3$)")
            plt.ylabel("Distance")
            plt.xticks(rotation=45)
            plt.grid()

            plt.subplot(1, 2, 2)
            plt.plot(sorted_keys, [p * 100 for p in probs], 'b-o', label="Probability", linewidth=1.5, alpha=0.7)

            if overall_max_idx == early_max_idx:
                plt.plot(sorted_keys[early_max_idx], probs[early_max_idx] * 100, 'ro', markersize=8,
                     label=f'{sorted_keys[early_max_idx]} (The highest)', zorder=10)

            if overall_max_idx != early_max_idx:
                plt.plot(sorted_keys[overall_max_idx], probs[overall_max_idx] * 100, 'mo', markersize=8,
                         label=f'{sorted_keys[overall_max_idx]} (The highest)', zorder=11)
                plt.plot(sorted_keys[early_max_idx], probs[early_max_idx] * 100, 'ro', markersize=8,
                     label=f'{sorted_keys[early_max_idx]} (The applicable highest)', zorder=10)

            plt.title(f"Prediction probabilities - {name}")
            plt.xlabel(r"Time setting ($t_1\_t_2\_t_3$)")
            plt.ylabel("Probability(%)")
            plt.xticks(rotation=45)
            plt.grid()
            plt.legend()
            plt.tight_layout()

            save_path = os.path.join(save_dir, f"{name}_line_prediction.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"保存折线图 → {save_path}")
            plt.close()

            plt.figure(figsize=(15, 10))

            x = np.arange(len(sorted_keys))
            width = 0.35

            fig, ax1 = plt.subplots(figsize=(15, 8))
            ax2 = ax1.twinx()

            bars1 = ax1.bar(x - width/2, distances, width, color='blue', alpha=0.7, label='Distance')
            ax1.set_ylabel('Distance', color='blue')
            ax1.tick_params(axis='y', labelcolor='blue')

            prob_colors = ['orange'] * len(sorted_keys)

            if early_max_idx in range(len(sorted_keys)):
                prob_colors[early_max_idx] = 'red'

            if overall_max_idx != early_max_idx:
                prob_colors[overall_max_idx] = 'purple'

            bars2 = ax2.bar(x + width/2, [p * 100 for p in probs], width, color=prob_colors, alpha=0.7)
            ax2.set_ylabel('Probability (%)', color='orange')
            ax2.tick_params(axis='y', labelcolor='orange')
            ax2.set_ylim(0, 110)

            for i, d in enumerate(distances):
                ax1.text(x[i] - width/2, d + 0.05, f"{d:.4f}", ha='center', rotation=45, fontsize=8)

            for i, p in enumerate(probs):
                ax2.text(x[i] + width/2, p * 100 + 2, f"{p*100:.1f}%", ha='center', fontsize=8)

            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor='blue', alpha=0.7, label='Distance'),
                Patch(facecolor='orange', alpha=0.7, label='Probability'),

            ]
            if overall_max_idx == early_max_idx:
                legend_elements.append(Patch(facecolor='purple', alpha=0.7, label='The highest'))

            if overall_max_idx != early_max_idx:
                legend_elements.append(Patch(facecolor='purple', alpha=0.7, label='The applicable '))
                legend_elements.append(Patch(facecolor='red', alpha=0.7, label='The applicable highest'))



            ax1.legend(handles=legend_elements, loc='upper left')

            plt.title(f"Model prediction comparison for {name}")
            plt.xticks(x, sorted_keys, rotation=45)
            plt.xlabel('Time settings')
            plt.grid(axis='y', alpha=0.3)

            plt.tight_layout()

            save_path = os.path.join(save_dir, f"{name}_all_bars_prediction.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"保存所有柱状图 → {save_path}")
            plt.close()

        except Exception as e:
            print(f"处理图片对 {name} 时出错: {e}")

predict_all_images()